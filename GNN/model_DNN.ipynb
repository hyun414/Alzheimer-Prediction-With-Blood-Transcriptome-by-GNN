{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'BiS495/GNN'\n",
      "/work/home/nhkim/BiS495/GNN\n"
     ]
    }
   ],
   "source": [
    "cd BiS495/GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import adabound\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "\n",
    "# Env\n",
    "from utils import *\n",
    "from model_GAT import *\n",
    "from options import parse_args\n",
    "from test_model import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                 act_type: none                          \n",
      "               adj_thresh: 0.05                          \n",
      "                    alpha: 0.2                           \n",
      "               batch_size: 32                            \n",
      "                  cnv_dim: 0                             \n",
      "                  dropout: 0.2                           \n",
      "                 final_lr: 0.1                           \n",
      "                  gpu_ids: 3,4,5                         \n",
      "                input_dim: 1                             \n",
      "                label_dim: 2                             \n",
      "               lambda_cox: 1                             \n",
      "               lambda_nll: 1                             \n",
      "               lambda_reg: 0.0003                        \n",
      "            lin_input_dim: 958                           \n",
      "                       lr: 0.0001                        \n",
      "                lr_policy: linear                        \n",
      "                model_dir: ./pretrained_models           \n",
      "                  no_cuda: True                          \n",
      "               num_epochs: 50                            \n",
      "                 omic_dim: 10                            \n",
      "           optimizer_type: adam                          \n",
      "                 patience: 0.005                         \n",
      "              results_dir: ./results                     \n",
      "                     task: grad                          \n",
      "             weight_decay: 0.0005                        \n",
      "              which_layer: all                           \n",
      "----------------- End -------------------\n"
     ]
    }
   ],
   "source": [
    "opt = parse_args()\n",
    "# opt.lin_input_dim = 958\n",
    "# opt.lin_input_dim = 1437\n",
    "# opt.lin_input_dim = 1916\n",
    "opt.act_type = 'SM'\n",
    "opt.optimizer_type = 'adabound'\n",
    "opt.num_epochs = 100\n",
    "opt.lr = 0.00005\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_test(out, lb):\n",
    "    score = 0\n",
    "    out = out.flatten()\n",
    "    # print(out)\n",
    "    out = np.where(out > 0.5, 1.0, 0.0)\n",
    "    lb = np.array(lb.flatten())\n",
    "    # print(lb)\n",
    "    score += np.sum(out == lb)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc_score(out, lb):\n",
    "    out = out.flatten()\n",
    "    lb = np.array(lb.flatten())\n",
    "    # print(out)\n",
    "    # print(lb)\n",
    "    if isinstance(out, torch.Tensor):\n",
    "        out = out.detach().cpu().numpy()\n",
    "    if isinstance(lb, torch.Tensor):\n",
    "        lb = lb.detach().cpu().numpy()\n",
    "    # roc_auc_score는 실제 레이블과 예측 확률을 받아 AUROC 값을 반환\n",
    "    auroc = roc_auc_score(lb, out)\n",
    "    \n",
    "    return auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(labels):\n",
    "    n = len(labels)\n",
    "    one_hot = torch.zeros(n, 2)\n",
    "\n",
    "    for i in range(n):\n",
    "        if labels[i] == 0:\n",
    "            one_hot[i] = torch.tensor([1, 0])\n",
    "        elif labels[i] == 1:\n",
    "            one_hot[i] = torch.tensor([0, 1])\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=479, hidden_dim=[128, 64, 32, 8], output_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], hidden_dim[2])\n",
    "        self.fc4 = nn.Linear(hidden_dim[2], hidden_dim[3])\n",
    "        self.fc5 = nn.Linear(hidden_dim[3], output_dim)\n",
    "        self.relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        x = torch.softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader):\n",
    "    model = MLP()\n",
    "    model.to(device)\n",
    "    model.apply(initialize_weights)\n",
    "    max_acc = 0\n",
    "    all_outs = np.array([], dtype=np.int64)\n",
    "    all_labels = np.array([], dtype=np.int64)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = opt.lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(opt.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        running_auroc = 0.0\n",
    "        \n",
    "        for i, (features, labels) in enumerate(train_loader):\n",
    "            features = features.float().to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features = features.view(features.size(0), -1).to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "\n",
    "            # print('outputs: ', outputs)\n",
    "            # print('labels: ', labels)\n",
    "\n",
    "            labels_change = preprocess(labels)\n",
    "\n",
    "            loss = criterion(outputs, labels_change)\n",
    "            # loss.requires_grad = True\n",
    "            \n",
    "            #outs = torch.sigmoid(outputs)\n",
    "\n",
    "            outs = (outputs[:, 0] < 0.5).long()\n",
    "\n",
    "            acc = acc_test(outs, labels)\n",
    "            \n",
    "            all_outs = np.concatenate((all_outs, outs))\n",
    "            all_labels = np.concatenate((all_labels, labels.view(-1)))\n",
    "\n",
    "            # print('loss: ', loss)\n",
    "            # print('acc: ' , acc)\n",
    "            # print('auroc: ', auroc)\n",
    "            # print('grad: ', gradients)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_acc += acc.item()\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "            # print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "        auroc = auroc_score(all_outs, all_labels)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{opt.num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Acc: {running_acc/len(train_loader):.4f}, Auroc: {auroc:.4f}')\n",
    "        # print('outputs: ', outputs.view(-1))\n",
    "        # print('labels: ', labels.view(-1))\n",
    "        if running_acc > max_acc:\n",
    "            best_model = model\n",
    "            max_acc = running_acc\n",
    "\n",
    "    print('Training complete')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i, (features, labels) in enumerate(test_loader):\n",
    "            features = features.float().to(device)\n",
    "            features = features.view(features.size(0), -1).to(device)\n",
    "            \n",
    "            probs = model(features)\n",
    "\n",
    "            probs  = probs.cpu().detach().numpy()\n",
    "            predictions += probs.tolist()\n",
    "            print(len(predictions))\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(best_model, test_loader, te_labels):\n",
    "    model = best_model\n",
    "    preds_real = inference(model, test_loader, device)\n",
    "\n",
    "    preds_real = np.array(preds_real)\n",
    "    preds = (preds_real[:, 0] < 0.5).astype(int)\n",
    "    # print(preds_real)\n",
    "    # print(len(preds_real))\n",
    "    # print(preds)\n",
    "    # print(te_labels.view(-1))\n",
    "\n",
    "    acc = acc_test(preds, te_labels)\n",
    "    auroc = auroc_score(preds, te_labels)\n",
    "\n",
    "    print(f'Acc: {acc/len(te_labels):.4f}, Auroc: {auroc:.4f}')\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /work/home/nhkim/BiS495/GNN/input_features_labels/split4\n",
      "Training features and labels: torch.Size([576, 479, 1]) torch.Size([576, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing features and labels: torch.Size([145, 479, 1]) torch.Size([145, 1])\n",
      "Adjacency matrix: torch.Size([479, 479])\n",
      "Number of edges: tensor(43797)\n",
      "Epoch [1/100], Loss: 0.7361, Acc: 16.1111, Auroc: 0.4993\n",
      "Epoch [2/100], Loss: 0.7015, Acc: 16.6667, Auroc: 0.5089\n",
      "Epoch [3/100], Loss: 0.6921, Acc: 16.8889, Auroc: 0.5105\n",
      "Epoch [4/100], Loss: 0.6935, Acc: 16.8333, Auroc: 0.5144\n",
      "Epoch [5/100], Loss: 0.7014, Acc: 15.2222, Auroc: 0.5061\n",
      "Epoch [6/100], Loss: 0.6868, Acc: 16.7222, Auroc: 0.5090\n",
      "Epoch [7/100], Loss: 0.6910, Acc: 16.6667, Auroc: 0.5098\n",
      "Epoch [8/100], Loss: 0.6831, Acc: 18.5556, Auroc: 0.5183\n",
      "Epoch [9/100], Loss: 0.6811, Acc: 17.7222, Auroc: 0.5216\n",
      "Epoch [10/100], Loss: 0.6783, Acc: 18.5000, Auroc: 0.5268\n",
      "Epoch [11/100], Loss: 0.6806, Acc: 18.1667, Auroc: 0.5300\n",
      "Epoch [12/100], Loss: 0.6751, Acc: 18.0000, Auroc: 0.5321\n",
      "Epoch [13/100], Loss: 0.6735, Acc: 18.6111, Auroc: 0.5358\n",
      "Epoch [14/100], Loss: 0.6769, Acc: 18.3333, Auroc: 0.5380\n",
      "Epoch [15/100], Loss: 0.6739, Acc: 18.6667, Auroc: 0.5408\n",
      "Epoch [16/100], Loss: 0.6815, Acc: 17.2778, Auroc: 0.5405\n",
      "Epoch [17/100], Loss: 0.6783, Acc: 17.8333, Auroc: 0.5416\n",
      "Epoch [18/100], Loss: 0.6869, Acc: 17.3889, Auroc: 0.5416\n",
      "Epoch [19/100], Loss: 0.6641, Acc: 19.4444, Auroc: 0.5446\n",
      "Epoch [20/100], Loss: 0.6733, Acc: 18.2778, Auroc: 0.5458\n",
      "Epoch [21/100], Loss: 0.6722, Acc: 18.9444, Auroc: 0.5478\n",
      "Epoch [22/100], Loss: 0.6890, Acc: 16.9444, Auroc: 0.5469\n",
      "Epoch [23/100], Loss: 0.6687, Acc: 18.3333, Auroc: 0.5479\n",
      "Epoch [24/100], Loss: 0.6633, Acc: 21.0556, Auroc: 0.5526\n",
      "Epoch [25/100], Loss: 0.6598, Acc: 19.3333, Auroc: 0.5545\n",
      "Epoch [26/100], Loss: 0.6660, Acc: 19.6111, Auroc: 0.5567\n",
      "Epoch [27/100], Loss: 0.6559, Acc: 20.8889, Auroc: 0.5602\n",
      "Epoch [28/100], Loss: 0.6604, Acc: 19.5556, Auroc: 0.5619\n",
      "Epoch [29/100], Loss: 0.6641, Acc: 19.6667, Auroc: 0.5637\n",
      "Epoch [30/100], Loss: 0.6558, Acc: 20.2778, Auroc: 0.5659\n",
      "Epoch [31/100], Loss: 0.6589, Acc: 19.4444, Auroc: 0.5672\n",
      "Epoch [32/100], Loss: 0.6530, Acc: 20.1111, Auroc: 0.5691\n",
      "Epoch [33/100], Loss: 0.6554, Acc: 19.2778, Auroc: 0.5701\n",
      "Epoch [34/100], Loss: 0.6587, Acc: 19.6111, Auroc: 0.5713\n",
      "Epoch [35/100], Loss: 0.6551, Acc: 19.6667, Auroc: 0.5725\n",
      "Epoch [36/100], Loss: 0.6581, Acc: 19.2778, Auroc: 0.5733\n",
      "Epoch [37/100], Loss: 0.6525, Acc: 19.3889, Auroc: 0.5741\n",
      "Epoch [38/100], Loss: 0.6713, Acc: 18.5556, Auroc: 0.5742\n",
      "Epoch [39/100], Loss: 0.6602, Acc: 19.5556, Auroc: 0.5752\n",
      "Epoch [40/100], Loss: 0.6488, Acc: 20.6667, Auroc: 0.5769\n",
      "Epoch [41/100], Loss: 0.6490, Acc: 20.6111, Auroc: 0.5785\n",
      "Epoch [42/100], Loss: 0.6563, Acc: 19.2222, Auroc: 0.5790\n",
      "Epoch [43/100], Loss: 0.6575, Acc: 19.7222, Auroc: 0.5799\n",
      "Epoch [44/100], Loss: 0.6433, Acc: 20.3333, Auroc: 0.5812\n",
      "Epoch [45/100], Loss: 0.6530, Acc: 19.7222, Auroc: 0.5819\n",
      "Epoch [46/100], Loss: 0.6508, Acc: 19.5000, Auroc: 0.5825\n",
      "Epoch [47/100], Loss: 0.6517, Acc: 19.8333, Auroc: 0.5832\n",
      "Epoch [48/100], Loss: 0.6623, Acc: 18.6667, Auroc: 0.5832\n",
      "Epoch [49/100], Loss: 0.6499, Acc: 20.0000, Auroc: 0.5840\n",
      "Epoch [50/100], Loss: 0.6445, Acc: 20.3333, Auroc: 0.5850\n",
      "Epoch [51/100], Loss: 0.6430, Acc: 20.7778, Auroc: 0.5863\n",
      "Epoch [52/100], Loss: 0.6457, Acc: 20.5556, Auroc: 0.5873\n",
      "Epoch [53/100], Loss: 0.6480, Acc: 20.1111, Auroc: 0.5881\n",
      "Epoch [54/100], Loss: 0.6469, Acc: 19.2778, Auroc: 0.5884\n",
      "Epoch [55/100], Loss: 0.6527, Acc: 19.1667, Auroc: 0.5885\n",
      "Epoch [56/100], Loss: 0.6395, Acc: 20.7222, Auroc: 0.5895\n",
      "Epoch [57/100], Loss: 0.6422, Acc: 20.2222, Auroc: 0.5903\n",
      "Epoch [58/100], Loss: 0.6475, Acc: 19.8333, Auroc: 0.5908\n",
      "Epoch [59/100], Loss: 0.6405, Acc: 20.7778, Auroc: 0.5917\n",
      "Epoch [60/100], Loss: 0.6449, Acc: 20.0000, Auroc: 0.5923\n",
      "Epoch [61/100], Loss: 0.6481, Acc: 19.3889, Auroc: 0.5925\n",
      "Epoch [62/100], Loss: 0.6355, Acc: 20.7778, Auroc: 0.5934\n",
      "Epoch [63/100], Loss: 0.6395, Acc: 20.6667, Auroc: 0.5942\n",
      "Epoch [64/100], Loss: 0.6444, Acc: 19.9444, Auroc: 0.5947\n",
      "Epoch [65/100], Loss: 0.6360, Acc: 20.9444, Auroc: 0.5956\n",
      "Epoch [66/100], Loss: 0.6351, Acc: 21.2778, Auroc: 0.5966\n",
      "Epoch [67/100], Loss: 0.6410, Acc: 20.6111, Auroc: 0.5973\n",
      "Epoch [68/100], Loss: 0.6449, Acc: 19.6111, Auroc: 0.5975\n",
      "Epoch [69/100], Loss: 0.6417, Acc: 20.1667, Auroc: 0.5980\n",
      "Epoch [70/100], Loss: 0.6439, Acc: 19.5556, Auroc: 0.5982\n",
      "Epoch [71/100], Loss: 0.6350, Acc: 20.7222, Auroc: 0.5988\n",
      "Epoch [72/100], Loss: 0.6334, Acc: 21.1667, Auroc: 0.5997\n",
      "Epoch [73/100], Loss: 0.6316, Acc: 21.1111, Auroc: 0.6005\n",
      "Epoch [74/100], Loss: 0.6426, Acc: 19.7222, Auroc: 0.6007\n",
      "Epoch [75/100], Loss: 0.6401, Acc: 20.7778, Auroc: 0.6013\n",
      "Epoch [76/100], Loss: 0.6327, Acc: 21.1111, Auroc: 0.6021\n",
      "Epoch [77/100], Loss: 0.6397, Acc: 19.7222, Auroc: 0.6023\n",
      "Epoch [78/100], Loss: 0.6371, Acc: 20.8889, Auroc: 0.6029\n",
      "Epoch [79/100], Loss: 0.6357, Acc: 20.3889, Auroc: 0.6034\n",
      "Epoch [80/100], Loss: 0.6441, Acc: 20.0000, Auroc: 0.6036\n",
      "Epoch [81/100], Loss: 0.6365, Acc: 19.9444, Auroc: 0.6039\n",
      "Epoch [82/100], Loss: 0.6430, Acc: 20.4444, Auroc: 0.6043\n",
      "Epoch [83/100], Loss: 0.6331, Acc: 20.8333, Auroc: 0.6049\n",
      "Epoch [84/100], Loss: 0.6418, Acc: 20.1667, Auroc: 0.6051\n",
      "Epoch [85/100], Loss: 0.6392, Acc: 19.7778, Auroc: 0.6053\n",
      "Epoch [86/100], Loss: 0.6441, Acc: 19.8333, Auroc: 0.6054\n",
      "Epoch [87/100], Loss: 0.6510, Acc: 19.6667, Auroc: 0.6055\n",
      "Epoch [88/100], Loss: 0.6301, Acc: 21.2778, Auroc: 0.6062\n",
      "Epoch [89/100], Loss: 0.6390, Acc: 20.0000, Auroc: 0.6064\n",
      "Epoch [90/100], Loss: 0.6417, Acc: 19.6667, Auroc: 0.6064\n",
      "Epoch [91/100], Loss: 0.6390, Acc: 20.5000, Auroc: 0.6068\n",
      "Epoch [92/100], Loss: 0.6375, Acc: 20.5000, Auroc: 0.6072\n",
      "Epoch [93/100], Loss: 0.6503, Acc: 19.6667, Auroc: 0.6072\n",
      "Epoch [94/100], Loss: 0.6313, Acc: 20.9444, Auroc: 0.6078\n",
      "Epoch [95/100], Loss: 0.6309, Acc: 21.1111, Auroc: 0.6083\n",
      "Epoch [96/100], Loss: 0.6515, Acc: 19.6667, Auroc: 0.6083\n",
      "Epoch [97/100], Loss: 0.6284, Acc: 21.3889, Auroc: 0.6089\n",
      "Epoch [98/100], Loss: 0.6258, Acc: 21.7778, Auroc: 0.6097\n",
      "Epoch [99/100], Loss: 0.6293, Acc: 21.0556, Auroc: 0.6102\n",
      "Epoch [100/100], Loss: 0.6311, Acc: 21.0556, Auroc: 0.6106\n",
      "Training complete\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "145\n",
      "Acc: 0.6621, Auroc: 0.6496\n",
      "accuracy of DNN: 19.2000\n"
     ]
    }
   ],
   "source": [
    "acc_dnn = 0\n",
    "\n",
    "tr_features, tr_labels, te_features, te_labels, adj_matrix = load_csv_data(4, opt)\n",
    "train_dataset = TensorDataset(tr_features, tr_labels)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "te_dataset = TensorDataset(te_features, te_labels)\n",
    "test_loader = DataLoader(te_dataset, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "model = MLP()\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "best_model = train_model(train_loader)\n",
    "acc_dnn = acc_dnn + test_model(best_model, test_loader, te_labels)\n",
    "\n",
    "print(f\"accuracy of DNN: {acc_dnn/5.0:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_dnn = 0\n",
    "\n",
    "# for k in range(1, 6):\n",
    "#     tr_features, tr_labels, te_features, te_labels, adj_matrix = load_csv_data(k, opt)\n",
    "#     train_dataset = TensorDataset(tr_features, tr_labels)\n",
    "#     train_loader = DataLoader(dataset=train_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "#     te_dataset = TensorDataset(te_features, te_labels)\n",
    "#     test_loader = DataLoader(te_dataset, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "#     model = MLP()\n",
    "#     device = torch.device('cpu')\n",
    "#     model.to(device)\n",
    "#     model.apply(initialize_weights)\n",
    "\n",
    "#     best_model = train_model(train_loader)\n",
    "#     acc_dnn = acc_dnn + test_model(best_model, test_loader, te_labels)\n",
    "\n",
    "# print(f\"accuracy of DNN: {acc_dnn/5.0:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bis332-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
